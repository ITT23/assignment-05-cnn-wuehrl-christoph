{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ed4721-2c0e-4a78-888e-8c586cbd4b47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "# import a lot of things from keras:\n",
    "# sequential model\n",
    "from keras.models import Sequential\n",
    "\n",
    "# layers\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, RandomFlip, RandomRotation, RandomContrast, RandomBrightness\n",
    "\n",
    "# loss function\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "# callback functions\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# convert data to categorial vector representation\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# nice progress bar for loading data\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# helper function for train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import confusion matrix helper function\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# import pre-trained model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "# include only those gestures\n",
    "CONDITIONS = ['like', 'stop']\n",
    "\n",
    "# image size\n",
    "IMG_SIZE = 64\n",
    "SIZE = (IMG_SIZE, IMG_SIZE)\n",
    "\n",
    "# number of color channels we want to use\n",
    "# set to 1 to convert to grayscale\n",
    "# set to 3 to use color images\n",
    "COLOR_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b3f618-cd32-48f5-b43d-05d670ec3cba",
   "metadata": {},
   "source": [
    "## helper function to load and parse annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3705021c-b053-4b70-87b1-a0049ba7e6cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "annotations = dict()\n",
    "\n",
    "for condition in CONDITIONS:\n",
    "    with open(f'_annotations/{condition}.json') as f:\n",
    "        annotations[condition] = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4cdf6b-bad3-4fe1-b4c6-4471e8ffcdb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pretty-print first element\n",
    "print(json.dumps(annotations['like']['000484ab-5fd0-49b8-9253-23a22b71d7b1'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dad5afd-64ee-4dc2-9b34-4429210c3790",
   "metadata": {},
   "source": [
    "## helper function to pre-process images (color channel conversion and resizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6815af-85fb-483e-ae69-c3a3fee78ffa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_image(img):\n",
    "    if COLOR_CHANNELS == 1:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img_resized = cv2.resize(img, SIZE)\n",
    "    return img_resized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121f42a0-ece9-47f3-aefb-521366921c18",
   "metadata": {},
   "source": [
    "## load images and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06de9c48-aca0-468b-8048-bc7dca63d3ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images = [] # stores actual image data\n",
    "labels = [] # stores labels (as integer - because this is what our network needs)\n",
    "label_names = [] # maps label ints to their actual categories so we can understand predictions later\n",
    "\n",
    "# loop over all conditions\n",
    "# loop over all files in the condition's directory\n",
    "# read the image and corresponding annotation\n",
    "# crop image to the region of interest\n",
    "# preprocess image\n",
    "# store preprocessed image and label in corresponding lists\n",
    "for condition in CONDITIONS:\n",
    "    for filename in tqdm(os.listdir(condition)):\n",
    "        # extract unique ID from file name\n",
    "        UID = filename.split('.')[0]\n",
    "        img = cv2.imread(f'{condition}/{filename}')\n",
    "        \n",
    "        # get annotation from the dict we loaded earlier\n",
    "        try:\n",
    "            annotation = annotations[condition][UID]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "        \n",
    "        # iterate over all hands annotated in the image\n",
    "        for i, bbox in enumerate(annotation['bboxes']):\n",
    "            # annotated bounding boxes are in the range from 0 to 1\n",
    "            # therefore we have to scale them to the image size\n",
    "            x1 = int(bbox[0] * img.shape[1])\n",
    "            y1 = int(bbox[1] * img.shape[0])\n",
    "            w = int(bbox[2] * img.shape[1])\n",
    "            h = int(bbox[3] * img.shape[0])\n",
    "            x2 = x1 + w\n",
    "            y2 = y1 + h\n",
    "            \n",
    "            # crop image to the bounding box and apply pre-processing\n",
    "            crop = img[y1:y2, x1:x2]\n",
    "            preprocessed = preprocess_image(crop)\n",
    "            \n",
    "            # get the annotated hand's label\n",
    "            # if we have not seen this label yet, add it to the list of labels\n",
    "            label = annotation['labels'][i]\n",
    "            if label not in label_names:\n",
    "                label_names.append(label)\n",
    "            \n",
    "            label_index = label_names.index(label)\n",
    "            \n",
    "            images.append(preprocessed)\n",
    "            labels.append(label_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097ae97c-8dd4-48b7-8a94-bdc1fbe80346",
   "metadata": {},
   "source": [
    "## let's have a look at one of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b82f39-8412-4f5a-b074-d091fde3a88f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(random.sample(images, 1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27443d4b-83d1-43d8-9f23-52859703ebd3",
   "metadata": {},
   "source": [
    "## split data set into train and test\n",
    "\n",
    "x is for the actual data, y is for the label (this is convention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fdf1c6-1722-42d7-a353-69cfbdf6d675",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d160d5f-2bd9-4864-9a9e-d5525b55bde6",
   "metadata": {},
   "source": [
    "## transform data sets into a format compatible with our neural network\n",
    "\n",
    "image data has to be a numpy array with following dimensions: [image_id, y_axis, x_axis, color_channels]\n",
    "\n",
    "furthermore, scale all values to a range of 0 to 1\n",
    "\n",
    "training data has to be converted to a categorial vector (\"one hot\"):\n",
    "\n",
    "[3] --> [0, 0, 0, 1, 0, ..., 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e3d2c-37ad-4596-a431-cdfd55151145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = np.array(X_train).astype('float32')\n",
    "X_train = X_train / 255.\n",
    "\n",
    "X_test = np.array(X_test).astype('float32')\n",
    "X_test = X_test / 255.\n",
    "\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)\n",
    "\n",
    "train_label = y_train_one_hot\n",
    "test_label = y_test_one_hot\n",
    "\n",
    "X_train = X_train.reshape(-1, IMG_SIZE, IMG_SIZE, COLOR_CHANNELS)\n",
    "X_test = X_test.reshape(-1, IMG_SIZE, IMG_SIZE, COLOR_CHANNELS)\n",
    "\n",
    "print(X_train.shape, X_test.shape, train_label.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e2741-bb40-4501-819e-7a6c18ebcf17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# variables for hyperparameters\n",
    "batch_size = 8\n",
    "epochs = 50\n",
    "num_classes = len(label_names)\n",
    "activation = 'relu'\n",
    "activation_conv = 'LeakyReLU'  # LeakyReLU\n",
    "layer_count = 2\n",
    "num_neurons = 64\n",
    "\n",
    "# define model structure\n",
    "# with keras, we can use a model's add() function to add layers to the network one by one\n",
    "model = Sequential()\n",
    "\n",
    "# data augmentation (this can also be done beforehand - but don't augment the test dataset!)\n",
    "model.add(RandomFlip('horizontal'))\n",
    "model.add(RandomContrast(0.1))\n",
    "#model.add(RandomBrightness(0.1))\n",
    "#model.add(RandomRotation(0.2))\n",
    "\n",
    "# first, we add some convolution layers followed by max pooling\n",
    "model.add(Conv2D(64, kernel_size=(9, 9), activation=activation_conv, input_shape=(SIZE[0], SIZE[1], COLOR_CHANNELS), padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(4, 4), padding='same'))\n",
    "\n",
    "model.add(Conv2D(32, (5, 5), activation=activation_conv, padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), padding='same'))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation=activation_conv, padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "\n",
    "# dropout layers can drop part of the data during each epoch - this prevents overfitting\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# after the convolution layers, we have to flatten the data so it can be fed into fully connected layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# add some fully connected layers (\"Dense\")\n",
    "for i in range(layer_count - 1):\n",
    "    model.add(Dense(num_neurons, activation=activation))\n",
    "\n",
    "model.add(Dense(num_neurons, activation=activation))\n",
    "\n",
    "# for classification, the last layer has to use the softmax activation function, which gives us probabilities for each category\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# specify loss function, optimizer and evaluation metrics\n",
    "# for classification, categorial crossentropy is used as a loss function\n",
    "# use the adam optimizer unless you have a good reason not to\n",
    "model.compile(loss=categorical_crossentropy, optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "# define callback functions that react to the model's behavior during training\n",
    "# in this example, we reduce the learning rate once we get stuck and early stopping\n",
    "# to cancel the training if there are no improvements for a certain amount of epochs\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0401f45-19a6-4447-805c-50516dcad753",
   "metadata": {},
   "source": [
    "## now, we can train the model using the fit() function\n",
    "## this will take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c4c865-ee8f-4385-8ae7-9d0f70802c9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    train_label,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test, test_label),\n",
    "    callbacks=[reduce_lr, stop_early]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaadcde-ab15-4e1c-bbab-cf359fba6325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's have a look at our model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30ac212-8bd0-4a2d-9f9e-4a137d020645",
   "metadata": {},
   "source": [
    "## Plot accuracy and loss of the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275092f5-84e2-4a26-850d-3b267f91fe27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "fig = plt.figure(figsize=(15, 7))\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy (Line), Loss (Dashes)')\n",
    "\n",
    "ax.axhline(1, color='gray')\n",
    "\n",
    "plt.plot(accuracy, color='blue')\n",
    "plt.plot(val_accuracy, color='orange')\n",
    "plt.plot(loss, '--', color='blue', alpha=0.5)\n",
    "plt.plot(val_loss, '--', color='orange', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63f0a8e-d384-4790-bb47-889224eb2130",
   "metadata": {},
   "source": [
    "## saving the model\n",
    "\n",
    "the function will create a directory for your model and save structure and weights in there\n",
    "\n",
    "sometimes you will see the .h5 format being used - even though this is a bit faster and needs less space, it comes with its limitations and isn't used that much any more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304f21c0-6fed-430b-9f96-1c75ccf5bee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('gesture_recognition')\n",
    "\n",
    "# and this is how you load the model\n",
    "# model = keras.models.load_model(\"gesture_recognition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165b3fd0-e2fd-4a10-95c6-1370908b9f0c",
   "metadata": {},
   "source": [
    "## visualize classification results with a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf19f75-976a-449a-bc7a-76a4d30e9c92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let the model make predictions for our training data\n",
    "y_predictions = model.predict(X_test)\n",
    "\n",
    "# we get a 2D numpy array with probabilities for each category\n",
    "print('before', y_predictions)\n",
    "\n",
    "# to build a confusion matrix, we have to convert it to classifications\n",
    "# this can be done by using the argmax() function to set the probability to 1 and the rest to 0\n",
    "y_predictions = np.argmax(y_predictions, axis=1)\n",
    "\n",
    "print('probabilities', y_predictions)\n",
    "\n",
    "# create and plot confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_predictions)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "ConfusionMatrixDisplay(conf_matrix, display_labels=label_names).plot(ax=plt.gca())\n",
    "\n",
    "plt.xticks(rotation=90, ha='center')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c5ebfc-d66d-4aa6-9738-2e2b2f7e8c94",
   "metadata": {},
   "source": [
    "## let's test our model in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96d94ec-6d88-4b25-92f0-663cfa423a83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "ret, frame = cap.read()\n",
    "if COLOR_CHANNELS == 1:\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "cap.release()\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51b9e16-b4a3-4017-bb1f-ce539e3ea61c",
   "metadata": {},
   "source": [
    "## manual cropping for demonstration purposes - you can do better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b80a4d-e59c-43ab-9581-705feff33eba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(frame[50:350, 60:210])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545e91f6-80b0-47a6-aaf7-fc6ef5b1762e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resized = cv2.resize(frame[50:350, 60:210], SIZE)#[20:350, 200:420], SIZE)\n",
    "plt.imshow(resized)\n",
    "resized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ae7ec6-6c50-4fe0-ab59-908e1123bdbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reshaped = resized.reshape(-1, IMG_SIZE, IMG_SIZE, COLOR_CHANNELS)\n",
    "reshaped.shape\n",
    "prediction = model.predict(reshaped)\n",
    "\n",
    "print(label_names[np.argmax(prediction)], np.max(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e143812-9d54-4677-8cc7-45039cc20827",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters, biases = model.layers[2].get_weights()\n",
    "print(filters.shape)\n",
    "\n",
    "fig, axes = plt.subplots(8, 8, figsize=(20, 20))\n",
    "\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        img = filters[:,:,:,i*8+j]\n",
    "        axes[i][j].imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0644e2-0739-42e8-a405-37c08f6d30e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(8, 8, figsize=(20, 20))\n",
    "\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        #axes[i][j].imshow(filters[:,:,:,i*8+j] * 255, 'gray')\n",
    "        kernel = filters[:,:,0,i*8+j]\n",
    "\n",
    "        #print(kernel.shape)\n",
    "\n",
    "        gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n",
    "        filtered = cv2.filter2D(gray, -1, kernel)\n",
    "\n",
    "        axes[i][j].imshow(filtered, 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7c9144-78d2-4725-944f-1bfd4fc1a97b",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "let's use a pre-trained model (VGG16) for our prediction\n",
    "\n",
    "note that VGG16 needs three color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a43fee9-6d18-4b32-807b-3e41d023a3cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epochs = 50\n",
    "num_classes = len(label_names)\n",
    "activation = 'relu'\n",
    "activation_conv = 'LeakyReLU'  # LeakyReLU\n",
    "layer_count = 2\n",
    "num_neurons = 256\n",
    "\n",
    "# load a VGG16 model trained on the imagenet dataset\n",
    "# include_top=False -> do not include the output layer\n",
    "# input_tensor -> tells the model about the dimensions of our images (VGG16 needs three color channels)\n",
    "# pooling -> which type of pooling to use between convolutions; max or avg seem to be the best\n",
    "VGG = VGG16(weights='imagenet', include_top=False, input_tensor=Input(shape=(IMG_SIZE, IMG_SIZE, COLOR_CHANNELS)), pooling='max')\n",
    "\n",
    "# we want to use the VGG's original weights -> make those layers untrainable\n",
    "for layer in VGG.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# build a new model and add the VGG layers\n",
    "model_vgg = Sequential()\n",
    "model_vgg.add(VGG)\n",
    "\n",
    "# append dense layers at the end\n",
    "for i in range(layer_count - 1):\n",
    "    model_vgg.add(Dense(num_neurons, activation=activation))\n",
    "model_vgg.add(Dropout(0.2))\n",
    "\n",
    "model_vgg.add(Dense(num_neurons, activation=activation))\n",
    "\n",
    "# classifier\n",
    "model_vgg.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_vgg.compile(loss=categorical_crossentropy, optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, min_lr=0.0001)\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cdd148-83c2-401a-a27f-57cc6b61dee8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_vgg = model_vgg.fit(\n",
    "    X_train,\n",
    "    train_label,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test, test_label),\n",
    "    callbacks=[reduce_lr, stop_early]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14479f5f-4af0-4506-aaf7-a08ee2fe8b5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77dd293-b9d0-48bd-ad2f-6f489316e8c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = history_vgg.history['loss']\n",
    "val_loss = history_vgg.history['val_loss']\n",
    "accuracy = history_vgg.history['accuracy']\n",
    "val_accuracy = history_vgg.history['val_accuracy']\n",
    "\n",
    "fig = plt.figure(figsize=(15, 7))\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy (Line), Loss (Dashes)')\n",
    "\n",
    "ax.axhline(1, color='gray')\n",
    "\n",
    "plt.plot(accuracy, color='blue')\n",
    "plt.plot(val_accuracy, color='orange')\n",
    "plt.plot(loss, '--', color='blue', alpha=0.5)\n",
    "plt.plot(val_loss, '--', color='orange', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0d4897-7d12-4ff1-9004-8b3d04d2df8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let the model make predictions for our training data\n",
    "y_predictions = model_vgg.predict(X_test)\n",
    "\n",
    "# we get a 2D numpy array with probabilities for each category\n",
    "print('before', y_predictions)\n",
    "\n",
    "# to build a confusion matrix, we have to convert it to classifications\n",
    "# this can be done by using the argmax() function to set the probability to 1 and the rest to 0\n",
    "y_predictions = np.argmax(y_predictions, axis=1)\n",
    "\n",
    "print('probabilities', y_predictions)\n",
    "\n",
    "# create and plot confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_predictions)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "ConfusionMatrixDisplay(conf_matrix, display_labels=label_names).plot(ax=plt.gca())\n",
    "\n",
    "plt.xticks(rotation=90, ha='center')\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2373dc1-5a13-462f-a732-f3fdbec12304",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
